---
title: "Jun23"
author: "Xinyu Zhang"
date: "6/23/2020"
output: html_document
---
```{r}
library(dplyr)
library(ggplot2)
av_AVQI_B <- read.csv("av_AVQI_B.csv")
av_AVQI_R <- read.csv("av_AVQI_R.csv")
#merging mean breahtiness and mean roughness into one table
av_AVQI_BR <-left_join(av_AVQI_B,av_AVQI_R, by = "dummy") %>% 
  glimpse
#plotting and fitting regression line
ggplot(data = av_AVQI_BR, aes(x = RoughMean, y = BreathyMean)) + 
  geom_point() +
  geom_smooth(method = "lm")
```

```{r}
#Comparing the above with the un-averaged data
merged <- read.csv("merged.csv")
library(ggplot2)
ggplot(data = merged, aes(x = Roughness, y = Breathiness, color = Subject)) + 
  geom_point() +
  geom_smooth(method = "lm")

#ggsave("roughness-breathiness.pdf", plot = last_plot(), device = NULL)
```

```{r}
#Only fitting one line over all points, regardless of subjects
merged <- read.csv("merged.csv")
library(ggplot2)
ggplot(data = merged, aes(x = Roughness, y = Breathiness)) + 
  geom_point(aes(color = Subject)) +
  geom_smooth(method = "lm")

#ggsave("roughness-breathiness.pdf", plot = last_plot(), device = NULL)
```

```{r}
#excluding the upper-right-most and lower-left-most squares
merged2 <- merged %>% 
  filter(!(Roughness > 750 & Breathiness > 750) & !(Roughness < 250 & Breathiness <250)) %>%
  glimpse

ggplot(data = merged2, aes(x = Roughness, y = Breathiness))+
         geom_point(aes(color = Subject))+
         geom_smooth(method = "lm")

#not much correlation here anymore
```

```{r}
library(dplyr)
#reading in listening test results
listener1 <- read.csv("Results_AK.csv", header = TRUE, sep = ";")
listener2 <- read.csv("Results_KS.csv", header = TRUE, sep = ";")
#renaming the two answers, removing column "Num"
SLP1 <- listener1 %>%
  rename(Roughness = Answer1, Breathiness = Answer2)%>%
  dplyr::select(-Num)

SLP2 <- listener2 %>%
  rename(Roughness = Answer1, Breathiness = Answer2)%>%
  dplyr::select(-Num)

#inserting a column with Speaker and T merged together in the listening experiment results table
#SLP1$dummy <- paste(SLP1$Speaker, "-", SLP1$T)
#SLP2$dummy <- paste(SLP2$Speaker, "-", SLP2$T)

#as_tibble(SLP1)
#as_tibble(SLP2)

#join the two listening results tables by matching both Speaker and T
SLP <-rbind(SLP1,SLP2) %>% 
  dplyr::select(-Z, -A) %>% 
  filter(!(Roughness > 750 & Breathiness > 750) & !(Roughness < 250 & Breathiness <250))

SLP$dummy <- paste(SLP$Speaker, "-", SLP$T)

str(SLP)
```

```{r}
AVQI <- read.csv("AVQI_results_new.csv")
#create a dummy column as an identifier for each "Speaker-T" entry
AVQI$dummy <- paste(AVQI$Speaker, "-", AVQI$T)

SLPRough <- SLP %>% 
  dplyr::select(Roughness, dummy)

#merging ("left-join" only appends AVQI entries that has SLP rating correspondents)
library(dplyr)
av_AVQI_R <-left_join(SLPRough,AVQI, by = "dummy")%>%
  dplyr::select(-Speaker, -T, -AVQI, -dummy)

#converting jitter into a factor
av_AVQI_R$Jitter <- as.numeric(levels(av_AVQI_R$Jitter))[av_AVQI_R$Jitter]

#have a look at the whole table
#av_AVQI_R
```

```{r}
SLPBreathy <- SLP %>% 
  dplyr::select(Breathiness, dummy)

#merging ("left-join" only appends AVQI entries that has SLP rating correspondents)
library(dplyr)
av_AVQI_B <-left_join(SLPBreathy,AVQI, by = "dummy")%>%
  dplyr::select(-Speaker, -T, -AVQI, -dummy)

#converting jitter into a factor
av_AVQI_B$Jitter <- as.numeric(levels(av_AVQI_B$Jitter))[av_AVQI_B$Jitter]

#av_AVQI_B
```

##Re-training the roughness tree

Partitioning:
```{r}
#setting seed for reproducibility
set.seed(1606)

#deviding the data into three parts

#setting proportions, 70% for training, devide the remainder into two halves
assignment <- sample(1:3, size = nrow(av_AVQI_R), prob = c(0.7, 0.15, 0.15), replace = TRUE)

#subsetting the data to training indices
roughness_train <- av_AVQI_R[assignment == 1, ]

#subsetting the data to validation indices
roughness_valid <- av_AVQI_R[assignment == 2, ]

#subsetting the data to test indices
roughness_test <- av_AVQI_R[assignment == 3, ]
```

Training the model:
```{r}
#the model
library(rpart)
roughness_model <- rpart(formula = Roughness ~.,
                         data = roughness_train,
                         method = "anova")

roughness_model
```

Plotting the tree:
```{r}
library(rpart.plot)
rpart.plot(x = roughness_model, yesno = 2, type = 0, extra = 0)
```

Evaluating the model:
```{r}
#generating preedictions on the test set
pred <- predict(object = roughness_model,
                newdata = roughness_test)

#computing the RMSE
library(Metrics)
rmse(actual = roughness_test$Roughness,
     predicted = pred)
```

Tuning the model:
```{r}
#plot the Complexity Parameter table
plotcp(roughness_model)

#print the "CP Table"
print(roughness_model$cptable)

#retreaving the optimal cp value based on cross-validation error
opt_index <- which.min(roughness_model$cptable[, "xerror"])
cp_opt <- roughness_model$cptable[opt_index, "CP"]

#prune the tree to optimazed cp value
roughness_model_opt <- prune(tree = roughness_model, cp = cp_opt)

#plot the optimized model
#rpart.plot(x = roughness_model_opt, yesno = 2, type = 0)
#^not plottable

#trying this:
rpart.plot(x = roughness_model_opt, type = 0)
```

Grid search:

Creating a grid of possible models:
```{r}
#establishing a list of possible values for minsplit and maxdepth
splits <- seq(1, 5, 1)
depths <- seq(1, 5, 1)

#creating a data frame containing all combinations
hyper_grid <- expand.grid(minsplit = splits, maxdepth = depths)

#checking out the grid
head(hyper_grid)

#printing the number of grid combinations
nrow(hyper_grid)

#number of potential models in the grid
num_models <- nrow(hyper_grid)

#creating an empty list to store models
roughness_models <- list()

#a for loop to loop over the rows of hyper_grid to train the grid of models

for (i in 1:num_models) {
  #getting minsplit, maxdepth values at row i
  minsplit <- hyper_grid$minsplit[i]
  maxdepth <- hyper_grid$maxdepth[i]
  
  #training a model and storing it in the list
  roughness_models[[i]] <- rpart(formula = Roughness ~ .,
                             data = roughness_valid,
                             method = "anova",
                             minsplit = minsplit,
                             maxdepth = maxdepth)
}
```

Evaluating the grid:
```{r}
#number of potential models in the grid
num_models <- lengths(roughness_models)

#creating an empty vector to store RMSE values
rmse_values <- c()

#a loop over the models to compute validation RMSE
for (i in 1:num_models){
  
  #retrieving the i^th model from the list
  model <- roughness_models[[i]]
  
  #generating predictions on roughness_valid
  pred <- predict(object = model,
                  newdata = roughness_valid)
  
  #computing validation RMSE and add to the vector
  rmse_values[i] <- rmse(actual = roughness_valid$Roughness,
                         predicted = pred)
}

#idenetifying the model with smallest validation set RMSE
best_model <- roughness_models[[which.min(rmse_values)]]

#printing the parameters of the best model
best_model$control

#computing test set RMSE on best_model
pred <- predict(object = best_model,
                newdata = roughness_test)
rmse(actual = roughness_test$Roughness,
     predicted = pred)
```

Plotting the new best model
```{r}
rpart.plot(best_model)
```

Training a bagged tree model:
```{r}
library(ipred)
#setting seed for reproducibility
set.seed(1557)

#training a bagged model
roughness_model_ipred <- bagging(formula = Roughness ~ .,
                           data = roughness_train,
                           coob = TRUE) #=using out-of-bag samples to estimate model accuracy

#printing the model
roughness_model_ipred

#calculating the rmse of the ipred model
pred <- predict(object = roughness_model_ipred,
                newdata = roughness_test)
                
#computing the rmse
roughness_ipred_rmse <- rmse(actual = roughness_test$Roughness,
     predicted = pred)

roughness_ipred_rmse
```

Assessing the bagged trees:
```{r}
#assess 10-25 bagged trees
ntree <- 10:50

#create empty vector to store out of bag RMSE values
rmse <- vector(mode = "numeric", length = length(ntree))

for (i in seq_along(ntree)) {
  #set seed for reproducibility
  set.seed(1555)
  
  #perform bagged model
  model <- bagging(
    formula = Roughness ~ .,
    data = roughness_train,
    coob = TRUE,
    nbagg = ntree[i]
  )
  #get out of bag error
  rmse[i] <- model$err
}

plot(ntree, rmse, type = 'l', lwd = 2)
abline(v = 45, col = "blue", lty = "dashed")
```

^the lowest rmse appeared when the number of trees is 45.

Bagging with Caret:
```{r}
library(caret)
#specify a 10-fold cross validation
ctrl <- trainControl(method = "cv", number = 10)

#cross-validating the bagged model
set.seed(1753)
roughness_caret_model <- train(
  Roughness ~ .,
  data = roughness_train,
  method = "treebag",
  trControl = ctrl,
  #mtry = 45, #according to the above step
  #importance = TRUE,
  na.action = na.pass #ignore N/A's
)

plot(varImp(roughness_caret_model), 8)
```

Generate predictions from the caret model:
```{r}
pred <- predict(object = roughness_caret_model,
                newdata = roughness_test)
                #type = ""
#computing the rmse
roughness_caret_rmse <- rmse(actual = roughness_test$Roughness,
     predicted = pred)

roughness_caret_rmse
```

Comparing the test set performance to CV performance:
```{r}
#printing ipred::bagging test set rmse estimate
print(roughness_ipred_rmse)
print(roughness_caret_rmse)
roughness_caret_model$results[,"RMSE"] #average of the 10 models in the 10-fold cross validation
```

Random forests:
```{r}
library(randomForest)
library(randomForestExplainer)
#set.seed(1826)
#training the random forest
roughness_forest <- randomForest(formula = Roughness ~ ., data = roughness_train, na.action = na.omit, localImp = TRUE)

#print the model output
print(roughness_forest)

#explain_forest(roughness_forest)
```

Evaluating test set rmse
```{r}
#generating predictions on the test set
pred <- predict(object = roughness_forest,
                newdata = roughness_test,
                type = "response")
class(pred)

head(pred)

#computing the rmse
rmse(actual = roughness_test$Roughness,
     predicted = pred)
```

Tuning mtry
```{r}
#executing the tuning process
#set.seed(1920)
roughness_train <- roughness_train[complete.cases(roughness_train), ]
res <- tuneRF(x = subset(roughness_train, select = -Roughness),
              y = roughness_train$Roughness,
              ntreeTry = 500,
              doBest = TRUE)
res
```

Tuning a random forest via tree depth
```{r}
#Establishing a list of possible values for mtry, nodesize, and samplesize
mtry <- seq(4, ncol(roughness_train)*0.8, 2)
nodesize <- seq(3, 8, 2)
sampsize <- nrow(roughness_train) * c(0.7*0.8)

#creating a data frame containing all combinations
hyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)

#creating an empty vector to store OOB error values
oob_err <- c()

#loop over the rows of hype_frid to train the grid of models
for (i in 1:nrow(hyper_grid)) {
  #train a Random Forest model
  model <- randomForest(formula = Roughness ~ .,
                        data = roughness_train,
                        mtry = hyper_grid$mtry[i],
                        nodesize = hyper_grid$nodesize[i],
                        sampsize = hyper_grid$sampsize[i])
  #store OOB error for the model
  #oob_err[i] <- model$
  
}
```

```{r}
AVQI_RB

```

